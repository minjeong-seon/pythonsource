{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 라이브러리\n",
    "# requests, BeautifulSoup, Selenium, scrapy,...\n",
    "\n",
    "# 파이썬 라이브러리도 사용 가능하지만 외부 라이브러리를 주로 사용함\n",
    "\n",
    "# requests : 특정 url 에서 정보를 가져오기 쉽게 도와주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "urls = [\"https://www.python.org/\", \"https://www.naver.com/\"]\n",
    "\n",
    "name = \"robots.txt\"\n",
    "\n",
    "for url in urls:\n",
    "    print(url+name)\n",
    "    robots = requests.get(url+name)\n",
    "    print(robots.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./google.html\n",
      "성공\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "\n",
    "url = \"https://google.com\"\n",
    "\n",
    "save_file = \"./google.html\"\n",
    "\n",
    "try:\n",
    "    file1, header1 = req.urlretrieve(url, save_file)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(file1)\n",
    "    print(\"성공\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# urlretrieve\n",
    "    - 요청하는 url의 정보를 로컬 파일로 저장\n",
    "    - 튜플로 반환\n",
    "    - csv, api 데이터 등 많은 양의 데이터를 한번에 저장할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file1 ./puppy.jpg\n",
      "file2 ./google.html\n"
     ]
    }
   ],
   "source": [
    "# 이미지, 웹사이트 가져오기\n",
    "img_url = \"https://img1.daumcdn.net/thumb/R1280x0.fjpg/?fname=http://t1.daumcdn.net/brunch/service/user/32E9/image/BA2Qyx3O2oTyEOsXe2ZtE8cRqGk.JPG\"\n",
    "html_url = \"https://www.google.com\"\n",
    "\n",
    "# 경로\n",
    "img_path = \"./puppy.jpg\"\n",
    "html_path = \"./google.html\"\n",
    "\n",
    "try:\n",
    "    file1, header1 = req.urlretrieve(img_url, img_path)\n",
    "    file2, header2 = req.urlretrieve(html_url, html_path)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"file1 {}\".format(file1))\n",
    "    print(\"file2 {}\".format(file2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다운로드는 필요 없고 내용만 분석하고 싶을 때\n",
    " - urlopen(), read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_url = \"https://www.kma.go.kr/wid/queryDFSRSS.jsp?zone=1138052000\"\n",
    "\n",
    "data = req.urlopen(weather_url).read()\n",
    "\n",
    "text = data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.11st.co.kr/browsing/BestSeller.tmall?method=getBestSellerMain&xfrom=main^gnb\"\n",
    "\n",
    "try:\n",
    "    res = req.urlopen(url)\n",
    "    contents = res.read().decode('euc-kr')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(res.info())\n",
    "    print(contents[:4000])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 뉴스 가져오기\n",
    "\n",
    "news_url = \"https://n.news.naver.com/article/417/0000932845?cds=news_media_pc&type=editn\"\n",
    "\n",
    "try:\n",
    "    res = req.urlopen(news_url)\n",
    "    contents = res.read().decode('utf-8')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(res.info())\n",
    "    print(contents[:4000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화진흥위원회 일별박스오피스 xml --> txt파일로 저장\n",
    "\n",
    "movie_url = \"https://kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.xml?key=f5eef3421c602c6cb7ea224104795888&targetDt=20230705\"\n",
    "\n",
    "try:\n",
    "    res = req.urlopen(movie_url)\n",
    "    contents = res.read().decode('utf-8')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    with open(\"./movie.txt\", \"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&\"\n",
    "\n",
    "param = {\"query\":\"뷔\"}\n",
    "\n",
    "url += urlencode(param)\n",
    "\n",
    "try:\n",
    "    res = req.urlopen(url)\n",
    "    contents = res.read().decode('utf-8')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(res.info())\n",
    "    print(contents[:4000])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fake-useragent : useragent 생성\n",
    "- 파이썬 크롤링 시 : 상대 서버에서 파이썬 프로그램으로 요청 들어온 사실 인지함\n",
    "- 브라우저에서 요청하는 것처럼 속이는 방법임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mozilla/4.0 (compatible; MSIE 5.17; Mac_PowerPC Mac OS; en)\n",
      "Mozilla/5.0 (Windows; U; Windows NT 5.2; en-US) AppleWebKit/534.10 (KHTML, like Gecko) Chrome/7.0.540.0 Safari/534.10\n",
      "Opera/9.80 (X11; Linux i686; U; pl) Presto/2.2.15 Version/10.00\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "userAgent = UserAgent()\n",
    "\n",
    "print(userAgent.ie) # internet explorer 형태로 접근\n",
    "print(userAgent.chrome) # chrome 형태로 접근\n",
    "print(userAgent.random) # 브라우저 random 형태로 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import urllib.request as req\n",
    "\n",
    "try:\n",
    "    userAgent = UserAgent()\n",
    "\n",
    "    url = \"https://v.daum.net/v/20230707045110818\"\n",
    "\n",
    "    headers = {\n",
    "        \"user-agent\": userAgent.chrome\n",
    "    }\n",
    "\n",
    "    request_url = req.Request(url, headers=headers)\n",
    "    info = req.urlopen(request_url)\n",
    "    res = info.read().decode(\"utf-8\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(\"response headers {}\".format(info.info()))\n",
    "    print(\"request headers {}\".format(request_url.header_items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순위: 1, 금액: 50000, 업체명: 카카오\n",
      "순위: 2, 금액: 70100, 업체명: 삼성전자\n",
      "순위: 3, 금액: 80900, 업체명: 금양\n",
      "순위: 4, 금액: 954000, 업체명: 에코프로\n",
      "순위: 5, 금액: 87100, 업체명: 삼성중공우\n",
      "순위: 6, 금액: 3680, 업체명: 삼부토건\n",
      "순위: 7, 금액: 28850, 업체명: KODEX 2차전지산업\n",
      "순위: 8, 금액: 17270, 업체명: 두산에너빌리티\n",
      "순위: 9, 금액: 31190, 업체명: KODEX 반도체\n",
      "순위: 10, 금액: 6250, 업체명: 시큐센\n"
     ]
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import urllib.request as req\n",
    "import json\n",
    "import csv\n",
    "\n",
    "kosdaq = []\n",
    "\n",
    "try:\n",
    "    userAgent = UserAgent()\n",
    "\n",
    "    url = \"https://finance.daum.net/api/search/ranks?limit=10\"\n",
    "\n",
    "    headers = {\n",
    "        \"user-agent\": userAgent.chrome,\n",
    "        \"referer\" : \"https://finance.daum.net/\"\n",
    "    }\n",
    "\n",
    "    request_url = req.Request(url, headers=headers)\n",
    "    info = req.urlopen(request_url)\n",
    "    res = info.read().decode(\"utf-8\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "else:\n",
    "    #print(res)  - json 형태로 다음 증권 인기검색 top10 정보 가져옴\n",
    "    data = json.loads(res)[\"data\"]\n",
    "\n",
    "    for item in data:\n",
    "        print(\"순위: {}, 금액: {}, 업체명: {}\".format(item[\"rank\"],item[\"tradePrice\"],item[\"name\"]))\n",
    "\n",
    "        kosdaq.append(item) # csv 파일로 저장하기 위해 리스트 안에 데이터 삽입\n",
    "\n",
    "        with open(\"./finace.csv\",\"w\",newline=\"\") as f:\n",
    "            output = csv.writer(f)\n",
    "            # 헤더명 작성\n",
    "            output.writerow(kosdaq[0].keys())\n",
    "\n",
    "            for row in kosdaq:\n",
    "                output.writerow(row.values())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requests 라이브러리를 이용한 크롤링\n",
    "- pip install requests\n",
    "- urllib.request 라이브러리보다 간단한 접근 방법 제공\n",
    "- decoding 적절하게 작업해줌\n",
    "- json 처리도 편함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 방식\n",
    "\n",
    "import requests\n",
    "\n",
    "# 세션을 이용한 요청 가능\n",
    "s = requests.session()\n",
    "\n",
    "r = s.get(\"https://www.naver.com\")\n",
    "\n",
    "# 데이터 제대로 받았는지 확인해보기\n",
    "print(r.status_code) # 200, 300, 400, 500 이런 거\n",
    "print(r.ok)     # 200 = ok --> true  반환\n",
    "\n",
    "print(r.text)\n",
    "\n",
    "# 세션 종료\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get / post / put / delete 함수 제공\n",
    "\n",
    "import requests\n",
    "\n",
    "s = requests.session()\n",
    "\n",
    "r = s.get(\"https://httpbin.org/get\")\n",
    "\n",
    "\n",
    "print(r.status_code) \n",
    "print(r.ok)   \n",
    "\n",
    "print(r.text)\n",
    "\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "s = requests.session()\n",
    "\n",
    "# post + data\n",
    "data = {\n",
    "    \"name\":\"kim\"\n",
    "}\n",
    "\n",
    "r = s.post(\"https://httpbin.org/post\",data=data)\n",
    "\n",
    "\n",
    "print(r.status_code) \n",
    "print(r.ok)   \n",
    "\n",
    "print(r.text)\n",
    "\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "s = requests.session()\n",
    "\n",
    "param = {\"name\":\"kim\",\"age\":16}\n",
    "\n",
    "r = s.post(\"https://httpbin.org/post\",data=data)\n",
    "\n",
    "\n",
    "print(r.status_code) \n",
    "print(r.ok)   \n",
    "\n",
    "print(r.text)\n",
    "\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}\n",
      "dict_keys(['userId', 'id', 'title', 'completed'])\n",
      "dict_values([1, 1, 'delectus aut autem', False])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "r=requests.get(\"https://jsonplaceholder.typicode.com/todos/1\")\n",
    "\n",
    "# print(r.text)\n",
    "print(r.json())\n",
    "print(r.json().keys())\n",
    "print(r.json().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://jsonplaceholder.typicode.com/users\")\n",
    "\n",
    "for row in r.json():\n",
    "    for k, v in row.items():\n",
    "        print(k, \":\",v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"\n",
    "https://shoppinghow.kakao.com/siso/p/api/bestRank/vCateInfo?_=1688698734091\")\n",
    "\n",
    "for idx, product in enumerate(r.json(), start=1):\n",
    "    print(idx, product[\"product_name\"], product[\"price_\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonsource",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
